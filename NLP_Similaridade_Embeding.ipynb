{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Mgum7LZZmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c56f423c-e2f5-47b7-c827-2beb1f382f9c"
      },
      "source": [
        "!pip install afinn\n",
        "!python -m textblob.download_corpora\n",
        "!pip install -U textblob\n",
        "!pip install vaderSentiment\n",
        "!pip install pyemd\n",
        "!pip install unidecode\n",
        "!pip install pandas\n",
        "!pip install gensim\n",
        "!pip install sklearn\n",
        "!pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: afinn in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (0.1)\n",
            "Finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: textblob in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (0.15.3)\n",
            "Requirement already satisfied, skipping upgrade: nltk>=3.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from textblob) (3.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from nltk>=3.1->textblob) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: regex in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from nltk>=3.1->textblob) (2020.7.14)\n",
            "Requirement already satisfied, skipping upgrade: click in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from nltk>=3.1->textblob) (4.48.2)\n",
            "Requirement already satisfied: vaderSentiment in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from vaderSentiment) (2.18.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->vaderSentiment) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->vaderSentiment) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->vaderSentiment) (2018.4.16)\n",
            "Requirement already satisfied: pyemd in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (0.5.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from pyemd) (1.19.1)\n",
            "Requirement already satisfied: unidecode in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from pandas) (1.19.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from pandas) (2020.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.11.0)\n",
            "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from gensim) (1.19.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: Cython==0.29.14 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from gensim) (0.29.14)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: boto in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.48)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2018.4.16)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.48)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: sklearn in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from sklearn) (0.23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from scikit-learn->sklearn) (1.19.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (3.3.1)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: Cannot uninstall 'certifi'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting certifi>=2020.06.20\n",
            "  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from matplotlib) (1.19.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from matplotlib) (7.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
            "Installing collected packages: certifi\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2018.4.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRKRSwKrQqL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91f22785-4e38-471d-acc5-88282f3e62df"
      },
      "source": [
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (1.19.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRjtnTdTo0DD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ceaf2009-c6c2-475d-def5-edb25835f205"
      },
      "source": [
        "!pip install -U numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: numpy in c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages (1.19.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR9dcTSYZZml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "db91d9e7-5638-4937-b3e6-db2074194968"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "import bz2\n",
        "import gensim\n",
        "import warnings\n",
        "import numpy as np\n",
        "from gensim.models import word2vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "tqdm_notebook.pandas()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\user\\appdata\\roaming\\python36\\lib\\site-packages\\ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZl84nm5VQAA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "815cb2aa-4be2-418e-ec62-3addfe45477f"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# packages in environment at c:\\users\\user\\appdata\\roaming\\python36:\n",
            "#\n",
            "# Name                    Version                   Build  Channel\n",
            "matplotlib                3.3.1                         0  \n",
            "matplotlib-base           3.3.1            py36h4c87cce_0  \n",
            "\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSCC3v-rKxQv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrmWrr3gWaXu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjOYV0cfZZmq"
      },
      "source": [
        "# Carregando os embeddings\n",
        "\n",
        "Aqui vamos utilizar os embeddings para realizar as seguintes atividades:\n",
        "\n",
        "- análise de simlaridade\n",
        "- classificação de documentos\n",
        "\n",
        "<b> Carregue os embeddings treinados, como vimos na Aula 2. É o mesmo arquivo que iremos utilizar</b>\n",
        "\n",
        "Link: https://drive.google.com/open?id=1zI8pGfbUHuU_0wY_FV4tD6w6ZCUJTQbh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqMDMgrBZZmr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoRXbSwaxKvX"
      },
      "source": [
        "newfilepath = \"embedding_wiki_100d_pt.txt\"\n",
        "filepath = \"ptwiki_20180420_100d.txt.bz2\"\n",
        "with open(newfilepath, 'wb') as new_file, bz2.BZ2File(filepath, 'rb') as file:\n",
        "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
        "        new_file.write(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88ZGJzDjHi0Q"
      },
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(newfilepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqZJuktuZZmv"
      },
      "source": [
        "# Similaridade de Documentos\n",
        "\n",
        "Para realizar a similaridade entre documentos, utilize as frases abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-YjfkcTZZmw"
      },
      "source": [
        "frase1 = \"Excelente produto chegou antes do prazo indico e recomendo produto bom pois já testei e foi mais que aprovado\"\n",
        "frase2 = \"SUPER RECOMENDO, PREÇO, QUALIDADE #BRASTEMP, EFICIÊNCIA NA ENTREGA, E FACILIDADE DE PAGAMENTO. MUITO BOM!!!\"\n",
        "frase3 = \"A tampa do fogão veio com problemas com o pino de encaixe solto e precisa de reparos\"\n",
        "frase4 = \"Fogão ótimo!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNGfjS5RZZm1"
      },
      "source": [
        "## Distância de Jaccard\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "1) Faça um método que calcule a similaridade de Jaccard e aplique para os seguintes pares de frases:\n",
        "\n",
        "- Frase1 e Frase2\n",
        "- Frase1 e Frase3\n",
        "- Frase2 e Frase3\n",
        "- Frase1 e Frase4\n",
        "\n",
        "Observação: lembrando que você precisa aplicar um pre-processamento nessas frases antes de aplicar o método.\n",
        "Faça:\n",
        "\n",
        "- Lower\n",
        "- Remoção StopWords\n",
        "- Remoção Pontuação\n",
        "- Tokenização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn2YYL7bZZm2"
      },
      "source": [
        "def pre_processamento_texto(corpus):\n",
        "\n",
        "    #token\n",
        "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
        "\n",
        "    #aplicar lower\n",
        "    corpus_alt = [t.lower() for t in corpus_alt]\n",
        "\n",
        "    #remove stops\n",
        "    #portuguese_stops = stopwords.words('portuguese')\n",
        "    #corpus_alt = [p for p in corpus_alt if p not in portuguese_stops]\n",
        "\n",
        "    #remove numeros\n",
        "    corpus_alt = [re.sub(r'\\d', '', p) for p in corpus_alt]\n",
        "\n",
        "    #remove pontuacoes\n",
        "    corpus_alt = [p for p in corpus_alt if p not in string.punctuation]\n",
        "\n",
        "    #remove acentos\n",
        "    corpus_alt = [unidecode(p) for p in corpus_alt]\n",
        "\n",
        "    return corpus_alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAoMA-Zkzh2V"
      },
      "source": [
        "frase1_preprocessada = pre_processamento_texto(frase1)\n",
        "frase2_preprocessada = pre_processamento_texto(frase2)\n",
        "frase3_preprocessada = pre_processamento_texto(frase3)\n",
        "frase4_preprocessada = pre_processamento_texto(frase4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlKSdd2L0HKE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "ec08e72c-3cf1-424e-a5c4-a4f814e2d497"
      },
      "source": [
        "frase1_preprocessada"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['excelente',\n",
              " 'produto',\n",
              " 'chegou',\n",
              " 'antes',\n",
              " 'do',\n",
              " 'prazo',\n",
              " 'indico',\n",
              " 'e',\n",
              " 'recomendo',\n",
              " 'produto',\n",
              " 'bom',\n",
              " 'pois',\n",
              " 'ja',\n",
              " 'testei',\n",
              " 'e',\n",
              " 'foi',\n",
              " 'mais',\n",
              " 'que',\n",
              " 'aprovado']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWZMUzT5cBx6"
      },
      "source": [
        "def jaccard(f1, f2):\n",
        "    return len(set(f1).intersection(set(f2)))/len(set(f1).union(set(f2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XeBfxcIcCGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d8becdff-2d87-4a9b-fa00-6743b791c75f"
      },
      "source": [
        "print('Frase 1 e 2 = ', jaccard(frase1_preprocessada, frase2_preprocessada))\n",
        "print('Frase 1 e 3 = ', jaccard(frase1_preprocessada, frase3_preprocessada))\n",
        "print('Frase 2 e 3 = ', jaccard(frase2_preprocessada, frase3_preprocessada))\n",
        "print('Frase 1 e 4 = ', jaccard(frase1_preprocessada, frase4_preprocessada))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frase 1 e 2 =  0.10714285714285714\n",
            "Frase 1 e 3 =  0.06666666666666667\n",
            "Frase 2 e 3 =  0.07407407407407407\n",
            "Frase 1 e 4 =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke1PVggcZZm8"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "2) Qual par de frase teve maior simlaridade? E qual teve menor? Este resultado faz sentido? Explique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snOzG2EaSkw4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRt4SWRlSt0l"
      },
      "source": [
        "A maior similaridade foi entre as frases 1 e 2 com o valor de: 0.10714285714285714\n",
        "A menor similaridade foi entre as frases 1 e 4 com valor 0\n",
        "Faz sentido pois na frase 4 apenas foi dito Fogão ótimo, nas outras frases tiveram mais detalhes e mais chance de serem similares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND3usAELZZm-"
      },
      "source": [
        "## Distância de Cosseno\n",
        "\n",
        "Aqui iremos calcular a distância do cosseno utilizando duas formas, que aprendemos na aula passada, para representar o texto.\n",
        "\n",
        "- Bag of Words (BOW)\n",
        "- Embedding\n",
        "\n",
        "Observação:\n",
        "\n",
        "Existem duas formas de trabalhar com o cosseno:\n",
        "\n",
        "<b> Distância </b>: quanto menor mais perto estão as frases.\n",
        "<b> Similaridade </b>: quanto maior mais perto estão as frases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Qw93ZLZZm-"
      },
      "source": [
        "### BOW - Distância do cosseno\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "3) Calcule a distância do cosseno utilizando a representação CountVectorizer e aplique para os seguintes pares de frases:\n",
        "\n",
        "- Frase1 e Frase2\n",
        "- Frase1 e Frase3\n",
        "- Frase2 e Frase3\n",
        "- Frase1 e Frase4\n",
        "\n",
        "Observação: no CountVectorizer utilizem as frases já pre-processadas da atividade anterior. Mas para aplicá-las no fit_transform, cada frase deve ser um string (sem estar tokenizada) dentro de uma lista.\n",
        "\n",
        "```python\n",
        "#exemplo\n",
        "distance.cosine(frase1, frase2)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CzqpPPGZZm_"
      },
      "source": [
        "vetor = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzKIVGEKcNX7"
      },
      "source": [
        "frases_bow = vetor.fit_transform([\" \".join(frase1_preprocessada),\n",
        "                     \" \".join(frase2_preprocessada),\n",
        "                     \" \".join(frase3_preprocessada),\n",
        "                     \" \".join(frase4_preprocessada),])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82uwTpD1cNg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "96cce185-7c27-4f22-ac02-e823eb373005"
      },
      "source": [
        "frases_bow.todense()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "         0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "         1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viuDlgoicNod"
      },
      "source": [
        "frase1_bow = frases_bow.todense()[0]\n",
        "frase2_bow = frases_bow.todense()[1]\n",
        "frase3_bow = frases_bow.todense()[2]\n",
        "frase4_bow = frases_bow.todense()[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7WAw9u7cNv1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "39a09042-8e0e-402f-dc8f-13c39ebf6219"
      },
      "source": [
        "print(distance.cosine(frase1_bow, frase2_bow))\n",
        "print(distance.cosine(frase1_bow, frase3_bow))\n",
        "print(distance.cosine(frase2_bow, frase3_bow))\n",
        "print(distance.cosine(frase1_bow, frase4_bow))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8727430474048444\n",
            "0.9459261929564124\n",
            "0.8692559099078774\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcFS1MKzZZnE"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "\n",
        "4) Qual par de frase teve maior distância? E qual teve menor? Este resultado faz sentido? Explique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJQy_wnMTSSu"
      },
      "source": [
        "Maior distância entre as frases 1 e 4\n",
        "Menor distância entre as frases 2 e 3\n",
        "Faz sentido pois quanto menor a distância maior a similaridade entre as frases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buNHHZPgZZnF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3G4ckzlZZnN"
      },
      "source": [
        "### Embedding - Distância do cosseno\n",
        "\n",
        "Para calcular o embedding de cada uma das frases, utilize o modelo carregado inicialmente.\n",
        "\n",
        "Cada palavra tem um vetor, para formar o embedding da frase tire a média de todos os vetores.\n",
        "\n",
        "Utilize as frases já pre-processadas\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "5) Calcule a distância do cosseno utilizando a representação Embedding e aplique para os seguintes pares de frases:\n",
        "\n",
        "- Frase1 e Frase2\n",
        "- Frase1 e Frase3\n",
        "- Frase2 e Frase3\n",
        "- Frase1 e Frase4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRO8RnLgZZnO"
      },
      "source": [
        "def get_word2vec(frase):\n",
        "    return np.mean(np.array([word_vectors[p] for p in frase if p in word_vectors.vocab]), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLs5ssJRcXo9"
      },
      "source": [
        "frase1_emb = get_word2vec(frase1_preprocessada)\n",
        "frase2_emb = get_word2vec(frase2_preprocessada)\n",
        "frase3_emb = get_word2vec(frase3_preprocessada)\n",
        "frase4_emb = get_word2vec(frase4_preprocessada)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll4aSoHocXvf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fe5bb4ad-5f94-4dec-b417-c8b298a1daef"
      },
      "source": [
        "print(distance.cosine(frase1_emb, frase2_emb))\n",
        "print(distance.cosine(frase1_emb, frase3_emb))\n",
        "print(distance.cosine(frase2_emb, frase3_emb))\n",
        "print(distance.cosine(frase1_emb, frase4_emb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10306441783905029\n",
            "0.13053971529006958\n",
            "0.143882155418396\n",
            "0.35638290643692017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgZWHiGSZZnT"
      },
      "source": [
        "<b>Atividade </b>\n",
        "\n",
        "6) Qual par de frase teve maior distância? E qual teve menor? Este resultado faz sentido? Explique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsFFxOoeZZnU"
      },
      "source": [
        "Maior distância entre as frases 1 e 4\n",
        "Menor distância entre as frases 1 e 2\n",
        "Faz sentido pois quanto maior a distância menor a similaridade, contudo percebe-se que percentualmente o modelo embending teve valores diferentes do bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx5acLVhZZne"
      },
      "source": [
        "## WMD\n",
        "\n",
        "O WMD já está incorporado ao Word2Vec\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "7) Calcule a distância WMD e aplique para os seguintes pares de frases:\n",
        "\n",
        "- Frase1 e Frase2\n",
        "- Frase1 e Frase3\n",
        "- Frase2 e Frase3\n",
        "- Frase1 e Frase4\n",
        "\n",
        "Observação: use a variável já tokenizada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0881q7vZZne"
      },
      "source": [
        "from pyemd import emd\n",
        "from gensim.similarities import WmdSimilarity\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "from gensim.models.doc2vec import TaggedLineDocument"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoOFttNychNx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "636916a9-846f-43ac-ddb2-6ccf13077101"
      },
      "source": [
        "print(word_vectors.wmdistance(frase1_preprocessada, frase2_preprocessada))\n",
        "print(word_vectors.wmdistance(frase1_preprocessada, frase3_preprocessada))\n",
        "print(word_vectors.wmdistance(frase2_preprocessada, frase3_preprocessada))\n",
        "print(word_vectors.wmdistance(frase1_preprocessada, frase4_preprocessada))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6522280051386518\n",
            "2.8301634750747398\n",
            "2.8800446266377127\n",
            "4.148723259169023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsIENn25ZZni"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "8) Qual par de frase teve maior distância? E qual teve menor? Este resultado faz sentido? Explique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhC4OvGWZZnj"
      },
      "source": [
        "Maior distância entre as frases 1 e 4\n",
        "Menor distância entre as frases 1 e 2\n",
        "Novamente faz sentido o resultado já era esperado maior distância entre as frases 1 e 4\n",
        "Contudo novamente percebe-se uma diferença entre os métodos, que deve ser analisado de acordo com o tamanho do dataset e objetivo qual deve ser utilizado além da quantidade de features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXHGivtDZZnk"
      },
      "source": [
        "# Classificação de Documentos\n",
        "\n",
        "A clssificação de documentos é muito útil em vários aspectos. Um dos tipos de classificação de texto é a análise de sentimentos.\n",
        "\n",
        "A fim de ilustrar a classificação de documentos iremos criar um modelo para classificar uma frase como positiva ou negativa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhCOoEn7ZZno"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "9) Carregue o dataset com o pandas e depois dê o head no dataframe.\n",
        "\n",
        "\n",
        "Link download: https://drive.google.com/open?id=15azJWdEEPGsXQGiDmEOseTBJcquWvBQc\n",
        "\n",
        "<b> Este dataset é sobre revisões de filmes do IMDB. </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI0oXiOoZZnp"
      },
      "source": [
        "df = pd.read_csv(\"imdb-reviews-pt-br.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJYAOZecZZnv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9f3aba55-34c2-4eec-d26e-4176429a0136"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                            text_en  \\\n",
              "0   1  Once again Mr. Costner has dragged out a movie...   \n",
              "1   2  This is an example of why the majority of acti...   \n",
              "2   3  First of all I hate those moronic rappers, who...   \n",
              "3   4  Not even the Beatles could write songs everyon...   \n",
              "4   5  Brass pictures movies is not a fitting word fo...   \n",
              "\n",
              "                                             text_pt sentiment  \n",
              "0  Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
              "1  Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
              "2  Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
              "3  Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
              "4  Filmes de fotos de latão não é uma palavra apr...       neg  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text_en</th>\n",
              "      <th>text_pt</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
              "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAf1SLS3-QWt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e9d02752-90a2-430f-dd62-4cf601227045"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neg    24765\n",
              "pos    24694\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCKr5KPMZZnz"
      },
      "source": [
        "## Representação dos dados\n",
        "\n",
        "O sentimento positivo e negativo iremos binarizar cada um deles. Seja 1 positivo e 0 negativo.\n",
        "\n",
        "Iremos representar o texto de duas formas:\n",
        "\n",
        "- Bag of Words (BOW)\n",
        "- Embedding\n",
        "\n",
        "Depois iremos comparar o resultado de cada um deles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkSAiYMZZn0"
      },
      "source": [
        "### Representação Target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0mYbWgcZZn1"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "10) Faça a representação dos sentimentos. 1 positivo; 0 negativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBkDjo4VZZn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e9ec1b3f-5398-4a1f-f725-add017947933"
      },
      "source": [
        "target = df['sentiment'].replace(['neg', 'pos'], [0, 1])\n",
        "target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "49454    1\n",
              "49455    1\n",
              "49456    1\n",
              "49457    1\n",
              "49458    1\n",
              "Name: sentiment, Length: 49459, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8VgGgUnZZn8"
      },
      "source": [
        "### Bag of Words (BOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xirBWWCzZZn-"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "11) Aplique o pré-processamento listado abaixo na coluna ``text_pt`` (crie uma nova coluna ```text_pt_sem_stopwords``` no dataframe para armazenar este dado processado):\n",
        "\n",
        "- Remova as stopwords do texto\n",
        "- Remova as pontuções\n",
        "- Mantenha o texto sem tokenização, ou seja uma string\n",
        "\n",
        "<b> Dica: </b> use o ```progress_apply``` para exibir a barra de progresso:\n",
        "\n",
        "```python\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "df[\"colunas\"].progress_apply(lambda x: preprocessamento(x))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnJokm8VZZn-"
      },
      "source": [
        "def pre_processamento_texto_retorna_string(corpus):\n",
        "\n",
        "    #token\n",
        "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
        "\n",
        "    #aplicar lower\n",
        "    corpus_alt = [t.lower() for t in corpus_alt]\n",
        "\n",
        "    #remove stops\n",
        "    portuguese_stops = stopwords.words('portuguese')\n",
        "    corpus_alt = [p for p in corpus_alt if p not in portuguese_stops]\n",
        "\n",
        "    #remove numeros\n",
        "    corpus_alt = [re.sub(r'\\d', '', p) for p in corpus_alt]\n",
        "\n",
        "    #remove pontuacoes\n",
        "    corpus_alt = [p for p in corpus_alt if p not in string.punctuation]\n",
        "\n",
        "    #remove acentos\n",
        "    corpus_alt = [unidecode(p) for p in corpus_alt]\n",
        "\n",
        "    corpus_alt = ' '.join(corpus_alt)\n",
        "\n",
        "    return corpus_alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzGE_ZpBAGrg"
      },
      "source": [
        "df['texto_pt_limpo'] = df['text_pt'].apply(lambda x : pre_processamento_texto_retorna_string(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsWOL4eRCDcM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f29cbe5a-106c-45c9-d291-601f81f8bb9a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                            text_en  \\\n",
              "0   1  Once again Mr. Costner has dragged out a movie...   \n",
              "1   2  This is an example of why the majority of acti...   \n",
              "2   3  First of all I hate those moronic rappers, who...   \n",
              "3   4  Not even the Beatles could write songs everyon...   \n",
              "4   5  Brass pictures movies is not a fitting word fo...   \n",
              "\n",
              "                                             text_pt sentiment  \\\n",
              "0  Mais uma vez, o Sr. Costner arrumou um filme p...       neg   \n",
              "1  Este é um exemplo do motivo pelo qual a maiori...       neg   \n",
              "2  Primeiro de tudo eu odeio esses raps imbecis, ...       neg   \n",
              "3  Nem mesmo os Beatles puderam escrever músicas ...       neg   \n",
              "4  Filmes de fotos de latão não é uma palavra apr...       neg   \n",
              "\n",
              "                                      texto_pt_limpo  \n",
              "0  vez sr costner arrumou filme tempo necessario ...  \n",
              "1  exemplo motivo maioria filmes acao mesmos gene...  \n",
              "2  primeiro tudo odeio raps imbecis poderiam agir...  \n",
              "3  beatles puderam escrever musicas todos gostass...  \n",
              "4  filmes fotos latao palavra apropriada verdade ...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text_en</th>\n",
              "      <th>text_pt</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>texto_pt_limpo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
              "      <td>neg</td>\n",
              "      <td>vez sr costner arrumou filme tempo necessario ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
              "      <td>neg</td>\n",
              "      <td>exemplo motivo maioria filmes acao mesmos gene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
              "      <td>neg</td>\n",
              "      <td>primeiro tudo odeio raps imbecis poderiam agir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
              "      <td>neg</td>\n",
              "      <td>beatles puderam escrever musicas todos gostass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
              "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
              "      <td>neg</td>\n",
              "      <td>filmes fotos latao palavra apropriada verdade ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TddT2Y6JZZoC"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "12) Aplique a representação do texto processado anteriormente com CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axSZo69tZZoC"
      },
      "source": [
        "vector = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef2IMxAXCkR8"
      },
      "source": [
        "X_bow = vetor.fit_transform(df['texto_pt_limpo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owPIJZ39CkZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5608470b-7f05-4981-bf5b-3aaf4e7dec33"
      },
      "source": [
        "X_bow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<49459x125217 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 5028654 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mELsQ0tvZZoF"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSUe-Yr_ZZoG"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "13) Aplique o pré-processamento listado abaixo na coluna ``text_pt`` (crie uma nova coluna ```text_pt_sem_stopwords_token``` no dataframe para armazenar este dado processado):\n",
        "\n",
        "- Aplique lower\n",
        "- Remova as stopwords do texto\n",
        "- Remova as pontuções\n",
        "- Mantenha o texto com tokenização\n",
        "\n",
        "<b> Dica: </b> use o ```progress_apply``` para exibir a barra de progresso:\n",
        "\n",
        "```python\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "df[\"colunas\"].progress_apply(lambda x: preprocessamento(x))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBJxB_SsZZoH"
      },
      "source": [
        "def pre_processamento_texto_retorna_token(corpus):\n",
        "\n",
        "    #token\n",
        "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
        "\n",
        "    #aplicar lower\n",
        "    corpus_alt = [t.lower() for t in corpus_alt]\n",
        "\n",
        "    #remove stops\n",
        "    portuguese_stops = stopwords.words('portuguese')\n",
        "    corpus_alt = [p for p in corpus_alt if p not in portuguese_stops]\n",
        "\n",
        "    #remove numeros\n",
        "    corpus_alt = [re.sub(r'\\d', '', p) for p in corpus_alt]\n",
        "\n",
        "    #remove pontuacoes\n",
        "    corpus_alt = [p for p in corpus_alt if p not in string.punctuation]\n",
        "\n",
        "    #remove acentos\n",
        "    corpus_alt = [unidecode(p) for p in corpus_alt]\n",
        "\n",
        "\n",
        "    return corpus_alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSaW-0cPDYra"
      },
      "source": [
        "df['texto_pt_limpo_token'] = df['text_pt'].apply(lambda x : pre_processamento_texto_retorna_token(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBZaxuSkEDkT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "79afa502-e140-4957-effe-14ae8ed0ae32"
      },
      "source": [
        "df['texto_pt_limpo_token']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [vez, sr, costner, arrumou, filme, tempo, nece...\n",
              "1        [exemplo, motivo, maioria, filmes, acao, mesmo...\n",
              "2        [primeiro, tudo, odeio, raps, imbecis, poderia...\n",
              "3        [beatles, puderam, escrever, musicas, todos, g...\n",
              "4        [filmes, fotos, latao, palavra, apropriada, ve...\n",
              "                               ...                        \n",
              "49454    [media, votos, baixa, fato, funcionario, locad...\n",
              "49455    [enredo, algumas, reviravoltas, infelizes, ina...\n",
              "49456    [espantado, forma, filme, maioria, outros, med...\n",
              "49457    [christmas, together, realmente, veio, antes, ...\n",
              "49458    [drama, romantico, classe, trabalhadora, diret...\n",
              "Name: texto_pt_limpo_token, Length: 49459, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZPhMIrWZZoM"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "14) Aplique a representação do texto com Embeddings. Cada palavra tem um embedding, o embedding da frase é a média de todos embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4gg0S5NZZoM"
      },
      "source": [
        "def get_word2vec(frase):\n",
        "    return np.mean(np.array([word_vectors[p] for p in frase if p in word_vectors.vocab]), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFSB13_EV3N"
      },
      "source": [
        "X_word2vec = df['texto_pt_limpo_token'].apply(lambda x : get_word2vec(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3pZEsBfE_Nw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c0347e4a-f0e8-431a-da38-673d4dcc54c4"
      },
      "source": [
        "X_word2vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [0.15084998, 0.19326542, -0.056642305, -0.2193...\n",
              "1        [0.1662331, 0.22085148, -0.066288956, -0.20869...\n",
              "2        [0.22359823, 0.1167883, -0.01743333, -0.199178...\n",
              "3        [0.2631776, 0.12153541, -0.07142547, -0.212037...\n",
              "4        [0.21868937, 0.09785001, -0.07797347, -0.15642...\n",
              "                               ...                        \n",
              "49454    [0.17877448, 0.14892156, -0.08598039, -0.10364...\n",
              "49455    [0.17312221, 0.13372223, -0.026474074, -0.1776...\n",
              "49456    [0.18755417, 0.17317419, -0.056084007, -0.1863...\n",
              "49457    [0.18442088, 0.14988786, -0.10535385, -0.21823...\n",
              "49458    [0.27497926, 0.17049538, -0.053712655, -0.1587...\n",
              "Name: texto_pt_limpo_token, Length: 49459, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqPJf0yLZZoQ"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_xhW7e2ZZoQ"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpcc167FZZoR"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "15) Faça a divisão dados dados em treino e teste como no exemplo abaixo:\n",
        "\n",
        "```python\n",
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bag, target,random_state=123)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3MeGchRZZoS"
      },
      "source": [
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, target, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ggm8ZLZZoY"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "16) Treine com uma regressão logística"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJN8KMHiZZoa"
      },
      "source": [
        "modelo_bow = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys_3vcc-F3GO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24e86598-80a7-42f4-cfc9-10cede452b0f"
      },
      "source": [
        "modelo_bow.fit(X_train_bow, y_train_bow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vc_6etkF3TA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivlITfC2ZZof"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "17) Calcule as métricas de resultado utilizando método abaixo:\n",
        "\n",
        "```python\n",
        "print(classification_report(y_test_bow, y_pred))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsfRxqjSZZog"
      },
      "source": [
        "y_pred = modelo_bow.predict(X_test_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCJY_Q13GaE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "93b0c223-13f0-473c-b578-3a0743675e08"
      },
      "source": [
        "print(classification_report(y_test_bow, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.88      6112\n",
            "           1       0.87      0.89      0.88      6253\n",
            "\n",
            "    accuracy                           0.88     12365\n",
            "   macro avg       0.88      0.88      0.88     12365\n",
            "weighted avg       0.88      0.88      0.88     12365\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiulNTGNZZoj"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6abN65iqZZok"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "18) Faça a divisão dados dados em treino e teste como no exemplo abaixo:\n",
        "\n",
        "Verifique o shape do X treino e X teste. Caso eles estejam com apenas uma dimensão, você precisa tranformá-los para duas dimensões, caso contrário ocorrerá erro no treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn57b04_ZZol"
      },
      "source": [
        "X_train_embedding, X_test_embedding, y_train_embedding, y_test_embedding = \\\n",
        "train_test_split(X_word2vec.values, target, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9BR0H7fZZop"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "19) Treine com uma regressão logística"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJI29fY2ZZoq"
      },
      "source": [
        "modelo_embeding = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vldEOVDVJWNQ"
      },
      "source": [
        "X_train_embedding = pd.DataFrame([x for x in X_train_embedding])\n",
        "X_test_embedding = pd.DataFrame([x for x in X_test_embedding])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq4G1ymnIt-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e79f36-4a6c-42b0-da07-1062b4361ea2"
      },
      "source": [
        "modelo_embeding.fit(X_train_embedding, y_train_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvIYI1lJZZow"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "20) Calcule as métricas de resultado utilizando método abaixo:\n",
        "\n",
        "```python\n",
        "print(classification_report(y_test_bow, y_pred))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClFB32whZZox"
      },
      "source": [
        "#### Calcule as métricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-dRYhR_ZZoy"
      },
      "source": [
        "y_pred = modelo_embeding.predict(X_test_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1G25m05KF8N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "aa17c3c8-e66e-4143-c018-4fd16b609525"
      },
      "source": [
        "print(classification_report(y_test_embedding, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.76      0.76      6112\n",
            "           1       0.77      0.76      0.76      6253\n",
            "\n",
            "    accuracy                           0.76     12365\n",
            "   macro avg       0.76      0.76      0.76     12365\n",
            "weighted avg       0.76      0.76      0.76     12365\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNCy3MfMKF_8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sp62Sb1ZZo1"
      },
      "source": [
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "21) Compare os resultados obtidos com o BagOfWords e com o Embedding. Explique os possíveis motivos desta diferença."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RShZ0680ZZo2"
      },
      "source": [
        "Nesse caso acima tivemos o melhor resultado no modelo treinado usando BOW devido ao shape do dataset, o que nesse caso BOW tinha mais features e impactou num resultado melhor, porém nem sempre isso acontece deve-se atentar para riscos de overfinting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-oH0GQfZZo3"
      },
      "source": [
        "# Análise de sentimentos\n",
        "\n",
        "O modelo que criamos anteriormente é para ilustrar como podemos realizar classificação de documentos.\n",
        "Quando a tarefa é sobre análise de sentimentos, temos duas opções: treinar nosso próprio modelo, como feito anteriormente ou utilizar uma das inúmeras ferramentas prontas.\n",
        "\n",
        "Vamos testar as seguintes ferramentas:\n",
        "\n",
        "- Vader\n",
        "- Textblob\n",
        "- Affin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilpNXE0cZZo3"
      },
      "source": [
        "Nesta atividade iremos utilizar as duas variáveis abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq0dLI9JZZo4"
      },
      "source": [
        "texto_neg = df.loc[0, \"text_en\"]\n",
        "texto_pos = df.loc[49431, \"text_en\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dB-FuBgZZo9"
      },
      "source": [
        "## Vader\n",
        "\n",
        "<b> Apenas Inglês </b>\n",
        "\n",
        "O VADER (Valence Aware Dictionary e sEntiment Reasoner) é uma ferramenta de análise de sentimentos baseada em regras e léxico, especificamente identifica os sentimentos expressos nas mídias sociais.\n",
        "\n",
        "- positive sentiment: compound score >= 0.05\n",
        "- neutral sentiment: (compound score > -0.05) e (compound score < 0.05)\n",
        "- negative sentiment: compound score <= -0.05\n",
        "\n",
        "Mais informações: https://github.com/cjhutto/vaderSentiment\n",
        "\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "22) Aplique este método nas revisões ```texto_pos``` e ```texto_neg```.\n",
        "Para aplicar:\n",
        "\n",
        "```python\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "analyzer.polarity_scores(texto)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZE5KL-9ZZo9"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsgloUpZZpB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55f83976-4523-4553-c8a8-fba09e4c4ade"
      },
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "analyzer.polarity_scores(texto_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.084, 'neu': 0.737, 'pos': 0.179, 'compound': 0.9969}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vWsduIRV1u-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5bc3dfa-8e31-4433-9b47-854fdd1385a8"
      },
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "analyzer.polarity_scores(texto_neg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.126, 'neu': 0.76, 'pos': 0.114, 'compound': 0.3958}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDrJTCfKZZpE"
      },
      "source": [
        "## TextBlob\n",
        "\n",
        "<b> Apenas inglês </b>\n",
        "\n",
        "https://www.presentslide.in/2019/08/sentiment-analysis-textblob-library.html\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "23) Aplique este método nas revisões ```texto_pos``` e ```texto_neg```.\n",
        "Para aplicar:\n",
        "\n",
        "```python\n",
        "sentence=TextBlob(texto)\n",
        "sentence.sentiment\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9IvBg5sZZpE"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-BV6UkOZZpJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c536c65c-a5a6-4945-c005-25f71896e0c5"
      },
      "source": [
        "sentence=TextBlob(texto_pos)\n",
        "sentence.sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.190819118692253, subjectivity=0.6026226012793177)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qViq8y_JWR1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12a7965b-b09f-4622-8326-322716815ca7"
      },
      "source": [
        "sentence=TextBlob(texto_neg)\n",
        "sentence.sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.06385964912280702, subjectivity=0.5629824561403508)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwk7dHuFZZpO"
      },
      "source": [
        "## Afinn\n",
        "\n",
        "- Valor maior que 0 indica sentimento positivo\n",
        "- Valor menor que 0 indica sentimento negativo\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "24) Aplique este método nas revisões ```texto_pos``` e ```texto_neg```.\n",
        "Para aplicar:\n",
        "\n",
        "```python\n",
        "afinn = Afinn()\n",
        "afinn.score(texto)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2FS5AUsZZpP"
      },
      "source": [
        "from afinn import Afinn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF8E0CjzZZpX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcbb0cd7-37b7-4c39-fc59-6909f5625a9d"
      },
      "source": [
        "afinn = Afinn()\n",
        "afinn.score(texto_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3cnrLmFWiME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62a520c1-4fa1-463c-c7ea-b816bec61ef6"
      },
      "source": [
        "afinn = Afinn()\n",
        "afinn.score(texto_neg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkhkDVIxZZpa"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "25) Para você, qual ferramenta teve melhor comportamento?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tToksjxXnva"
      },
      "source": [
        "O Vader contribuiu bem na análise de texto positivo destacando maiores pesos em sentimento positivo e neutro, contudo não contribuiu para análise de texto negativo, ficando maior peso no neutro.\n",
        "\n",
        "O Textblob apresentou polaridade maior para o text_pos fortalecendo um sentimento mais positivo para o mesmo, porém a subjectividade ficou bem próxima nos dois textos, não sendo nesse caso a ferramenta mais indicada para análise.\n",
        "\n",
        "O Affin no meu ponto de vista fez mais sentido pois considerando que maior que zero indica sentimento positivo temos ambos os textos maior que zero, contudo o score do texto positivo é bem maior que o score do texto negativo, apontando assim um sentimento mais positivo no texto_pos ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWImUeS0ZZpb"
      },
      "source": [
        "# Dica:\n",
        "## Quando for trabalhar com um dataset em inglês, a biblioteca Spacy facilita!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONqxhviZZpc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "485e4a6f-dd1c-4ee6-d244-b29969585af6"
      },
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "#!pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Roaming\\Python36\\python.exe: No module named spacy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hj3KE97ZZph"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW0GLnhVZZpn"
      },
      "source": [
        "O scpay forne um pacote que já tem série de modelos já treinados em NLP. Inclusive para os embeddings em inglês.\n",
        "\n",
        "Para mais informações vá em:\n",
        "\n",
        "https://spacy.io/models/en#en_core_web_md\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v02Lid1QZZpo"
      },
      "source": [
        "Com o método abaixo carregamos um dos modelos do spacy:\n",
        "\n",
        "```python\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "```\n",
        "\n",
        "Para aplicar o modelo, basta passar o texto para o modelo carregado anteriormente:\n",
        "\n",
        "```python\n",
        "doc = nlp(\"This is some text that I am processing with Spacy\")\n",
        "```\n",
        "\n",
        "Carregue o modelo e imprima doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjRx4bZxZZpo"
      },
      "source": [
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDsD2THwZZps"
      },
      "source": [
        "doc = nlp(\"This is some text that I am processing with Spacy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDWkp1tfZZpy"
      },
      "source": [
        "doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuqCJaIMZZp3"
      },
      "source": [
        "Ao aplicar o modelo carregado a variável <b> doc </b> já possui os embeddings de cada uma das palavras e o embedding da frase, que é a média de todos vetores de todas palavras.\n",
        "\n",
        "```python\n",
        "#vetor da primeira palavra\n",
        "doc[0].vector\n",
        "#vetor agregado pela média - embedding do documento\n",
        "doc.vector\n",
        "```\n",
        "O código abaixo mostra que a média de uma posição em específico dos embeddings de todas as palavras e a posição do embedding do documento possuem o mesmo valor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r426C5mvZZp4"
      },
      "source": [
        "def calcula_media_posicao(x):\n",
        "    soma = 0\n",
        "    vector = []\n",
        "    for i in range(0,len(doc)):\n",
        "        vector.append(doc[i].vector)\n",
        "\n",
        "    for v in vector:\n",
        "        soma += v[x]\n",
        "    return soma/len(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "969rVBBYZZp8"
      },
      "source": [
        "round(calcula_media_posicao(10),6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmkeEtyQZZqB"
      },
      "source": [
        "round(doc.vector[10], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjZQ_jAMZZqF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}